
# ==== DEFAULT ENV FALLBACKS ADDED ====
import os as _os
_DEFAULT_ENV = {
    "DAMADAM_USERNAME": "your_username",
    "DAMADAM_PASSWORD": "your_password",
    "DAMADAM_USERNAME_2": "",
    "DAMADAM_PASSWORD_2": "",
    "GOOGLE_SHEET_URL": "https://docs.google.com/spreadsheets/d/xxxx/edit#gid=0",
    "GOOGLE_APPLICATION_CREDENTIALS": "credentials.json",
    "GOOGLE_CREDENTIALS_JSON": "",
    "MAX_PROFILES_PER_RUN": "0",
    "BATCH_SIZE": "20",
    "MIN_DELAY": "0.3",
    "MAX_DELAY": "0.5",
    "PAGE_LOAD_TIMEOUT": "30",
    "SHEET_WRITE_DELAY": "1.0",
}
for _k,_v in _DEFAULT_ENV.items():
    _os.environ.setdefault(_k,_v)
# ==== END DEFAULT ENV ====

#!/usr/bin/env python3
"""
DamaDam Target Bot - Single File v3.2.1

OVERVIEW:
  Automated bot to scrape DamaDam user profiles and store results in Google Sheets.
  Runs locally on Windows 10 or via GitHub Actions (scheduled every 1 hour).

WORKFLOW:
  1. Reads pending targets from 'Target' sheet (status: Pending or empty)
  2. Logs into DamaDam using provided credentials
  3. Scrapes profile data (gender, city, posts, followers, etc.)
  4. Appends new profiles to last row in 'ProfilesTarget' sheet
  5. Updates target status to 'Done' on success or 'Pending' on failure
  6. Applies Quantico font formatting to all data

KEY FEATURES:
  - Batch processing with adaptive delays to avoid API rate limits
  - Handles suspended/unverified accounts gracefully
  - Cookie-based session persistence
  - Google Sheets API integration with error recovery
  - Comprehensive logging with timestamps
  - Windows 10 compatible (no emoji encoding issues)

CONFIGURATION:
  Environment variables (see README.md):
    - DAMADAM_USERNAME, DAMADAM_PASSWORD (local defaults: 0utLawZ / asdasd)
    - GOOGLE_SHEET_URL, GOOGLE_APPLICATION_CREDENTIALS
    - MAX_PROFILES_PER_RUN, BATCH_SIZE, MIN_DELAY, MAX_DELAY, etc.

SCHEDULE:
  GitHub Actions: Every 1 hour (0 */1 * * *)
  Local: Run manually with: python Scraper.py
"""

# ==================== IMPORTS & CONFIG ====================

import warnings
import os, sys, re, time, json, random, argparse
from datetime import datetime, timedelta, timezone
from colorama import Fore, Style, init as colorama_init
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
from rich.progress import TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from rich.status import Status
colorama_init(autoreset=True)
console = Console()

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

import gspread
from google.oauth2.service_account import Credentials
from gspread.exceptions import WorksheetNotFound, APIError

warnings.filterwarnings("ignore", category=DeprecationWarning)

LOGIN_URL = "https://damadam.pk/login/"
HOME_URL = "https://damadam.pk/"
COOKIE_FILE = "damadam_cookies.pkl"

USERNAME = os.getenv('DAMADAM_USERNAME', '0utLawZ')  # Default for local testing
PASSWORD = os.getenv('DAMADAM_PASSWORD', 'asdasd')  # Default for local testing
USERNAME_2 = os.getenv('DAMADAM_USERNAME_2', '')
PASSWORD_2 = os.getenv('DAMADAM_PASSWORD_2', '')
GOOGLE_CREDENTIALS_RAW = os.getenv('GOOGLE_CREDENTIALS_JSON', '')
GOOGLE_SHEET_URL = os.getenv('GOOGLE_SHEET_URL', '').strip()
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMEDRIVER_PATH = os.getenv('CHROMEDRIVER_PATH', '').strip()
if not CHROMEDRIVER_PATH:
    CHROMEDRIVER_PATH = os.path.join(SCRIPT_DIR, 'chromedriver.exe')
GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS', '').strip()
if not GOOGLE_APPLICATION_CREDENTIALS:
    GOOGLE_APPLICATION_CREDENTIALS = 'credentials.json'

def _normalize_cred_path(p: str) -> str:
    p = (p or "").strip().strip('"').strip("'")
    if not p:
        return ""
    if os.path.isabs(p):
        return p
    return os.path.join(SCRIPT_DIR, p)

MAX_PROFILES_PER_RUN = int(os.getenv('MAX_PROFILES_PER_RUN', '0'))
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '20'))
APPLY_FONT_FORMATTING = os.getenv('APPLY_FONT_FORMATTING', '').strip().lower() in {"1","true","yes","y","on"}
MIN_DELAY = float(os.getenv('MIN_DELAY', '0.3'))
MAX_DELAY = float(os.getenv('MAX_DELAY', '0.5'))
PAGE_LOAD_TIMEOUT = int(os.getenv('PAGE_LOAD_TIMEOUT', '30'))
SHEET_WRITE_DELAY = float(os.getenv('SHEET_WRITE_DELAY', '1.0'))

COLUMN_ORDER = [
    "NICK NAME", "TAGS", "CITY", "GENDER", "MARRIED", "AGE", "JOINED", "FOLLOWERS", "STATUS", "POSTS", "INTRO", "SOURCE", "DATETIME SCRAP",
    "LAST POST", "LAST POST TIME", "IMAGE", "PROFILE LINK", "POST URL"
]
COLUMN_TO_INDEX = {name: idx for idx, name in enumerate(COLUMN_ORDER)}
COLUMN_TLOG_HEADERS = ["Timestamp", "Nickname", "Change Type", "Fields", "Before", "After"]
DASHBOARD_SHEET_NAME = "Dashboard"
HIGHLIGHT_EXCLUDE_COLUMNS = {"LAST POST", "LAST POST TIME", "JOINED", "PROFILE LINK", "DATETIME SCRAP"}
SUSPENSION_INDICATORS = [
    "accounts suspend",
    "aik se zyada fake accounts",
    "abuse ya harassment",
    "kisi aur user ki identity apnana",
    "accounts suspend kiye",
]
ENABLE_CELL_HIGHLIGHT = False

TARGET_STATUS_PENDING = "âš¡ Pending"
TARGET_STATUS_DONE = "Done ðŸ’€"
TARGET_STATUS_ERROR = "Error ðŸ’¥"

# ==================== HELPERS (TIME / TEXT / URL) ====================

IS_CI = bool(os.getenv('GITHUB_ACTIONS'))

def _print_rich(msg: str, style: str | None = None) -> None:
    if IS_CI:
        print(msg)
        sys.stdout.flush()
        return
    if style:
        console.print(msg, style=style)
    else:
        console.print(msg)

def get_pkt_time():
    return datetime.now(timezone.utc).replace(tzinfo=None) + timedelta(hours=5)

def log_msg(m):
    ts = get_pkt_time().strftime('%H:%M:%S')
    text = str(m)
    style = None
    icon = "â„¹ï¸"
    upper = text.upper()
    if "[OK]" in upper:
        style = "green"
        icon = "âœ…"
    elif "[ERROR]" in upper or "FATAL" in upper:
        style = "red"
        icon = "âŒ"
    elif "[SCRAPING]" in upper:
        style = "cyan"
        icon = "ðŸ•µï¸"
    elif "[TIMEOUT]" in upper:
        style = "yellow"
        icon = "â±ï¸"
    elif "[BROWSER_ERROR]" in upper:
        style = "red"
        icon = "ðŸ§¯"
    elif "[COMPLETE]" in upper:
        style = "magenta"
        icon = "ðŸ"

    if IS_CI:
        print(f"[{ts}] {text}")
        sys.stdout.flush()
        return

    _print_rich(f"[bold]{ts}[/bold] {icon} {text}", style=style)

def column_letter(i:int)->str:
    res=""; i+=1
    while i>0:
        i-=1; res=chr(i%26+65)+res; i//=26
    return res

def clean_data(v:str)->str:
    if not v: return ""
    v=str(v).strip().replace('\xa0',' ')
    bad={"No city","Not set","[No Posts]","N/A","no city","not set","[no posts]","n/a","[No Post URL]","[Error]","no set","none","null","no age"}
    return "" if v in bad else re.sub(r"\s+"," ", v)

def convert_relative_date_to_absolute(text:str)->str:
    if not text: return ""
    t=text.lower().strip().replace("mins","minutes").replace("min","minute").replace("secs","seconds").replace("sec","second").replace("hrs","hours").replace("hr","hour")
    m=re.search(r"(\d+)\s*(second|minute|hour|day|week|month|year)s?\s*ago", t)
    if not m: return text
    amt=int(m.group(1)); unit=m.group(2)
    s_map={"second":1,"minute":60,"hour":3600,"day":86400,"week":604800,"month":2592000,"year":31536000}
    if unit in s_map:
        dt=get_pkt_time()-timedelta(seconds=amt*s_map[unit]); return dt.strftime("%d-%b-%y")
    return text

def detect_suspension_reason(page_source:str)->str|None:
    if not page_source:
        return None
    lower=page_source.lower()
    for indicator in SUSPENSION_INDICATORS:
        if indicator in lower:
            return indicator
    return None

def calculate_eta(processed:int,total:int,start_ts:float)->str:
    if processed==0:
        return "Calculating..."
    elapsed=time.time()-start_ts
    rate=processed/elapsed if elapsed>0 else 0
    remaining=total-processed
    eta=remaining/rate if rate>0 else 0
    if eta<60:
        return f"{int(eta)}s"
    if eta<3600:
        return f"{int(eta//60)}m {int(eta%60)}s"
    hrs=int(eta//3600); mins=int((eta%3600)//60)
    return f"{hrs}h {mins}m"

def clean_text(text:str)->str:
    if not text: return ""
    text=str(text).strip().replace('\xa0',' ').replace('\n',' ')
    return re.sub(r"\s+"," ", text).strip()

def parse_post_timestamp(text:str)->str:
    return convert_relative_date_to_absolute(text)

def parse_owner_since_to_date(text:str)->str:
    text = text.strip()
    if "since" in text.lower():
        text = text.split("since")[1].strip()
        return convert_relative_date_to_absolute(text)
    return ""

def get_friend_status(driver) -> str:
    try:
        """
        // page ko lowercase karna safe matching ke liye zaroori
        """
        try:
            WebDriverWait(driver, 2).until(
                EC.presence_of_element_located(
                    (
                        By.CSS_SELECTOR,
                        "form[action*='/follow/remove/'], form[action*='/follow/add/'], img[src*='follow.svg'], img[src*='unfollow.svg'",
                    )
                )
            )
        except Exception:
            pass

        page = driver.page_source.lower()

        """
        // text original hi chahiye because FOLLOW / UNFOLLOW uppercase hota hai
        """
        text = driver.page_source

        try:
            imgs = driver.find_elements(By.CSS_SELECTOR, "img[src*='follow.svg'], img[src*='unfollow.svg']")
            has_unfollow_icon = False
            has_follow_icon = False
            for img in imgs:
                src = (img.get_attribute('src') or '').lower()
                if 'unfollow.svg' in src:
                    has_unfollow_icon = True
                elif re.search(r"(^|/)follow\.svg(\?|$)", src, re.IGNORECASE):
                    has_follow_icon = True
            if has_unfollow_icon:
                return "Yes"
            if has_follow_icon:
                return "No"
        except Exception:
            pass

        try:
            forms = driver.find_elements(By.CSS_SELECTOR, "form[action*='/follow/remove/'], form[action*='/follow/add/']")
            has_remove = False
            has_add = False
            has_unfollow_text = False
            has_follow_text = False
            for frm in forms:
                action = (frm.get_attribute('action') or '').lower()
                frm_text = (frm.text or '').lower()
                if '/follow/remove/' in action or action.endswith('/follow/remove'):
                    has_remove = True
                if '/follow/add/' in action or action.endswith('/follow/add'):
                    has_add = True
                if 'unfollow' in frm_text:
                    has_unfollow_text = True
                if re.search(r"\bfollow\b", frm_text, re.IGNORECASE) and 'unfollow' not in frm_text:
                    has_follow_text = True

            if has_remove or has_unfollow_text:
                return "Yes"
            if has_add or has_follow_text:
                return "No"
        except Exception:
            pass

        """
        // --- NOT FOLLOWING ---
        // FOLLOW text + /follow/add/ ka matlab user ne abhi follow nahi kiya
        """
        # NOTE: "UNFOLLOW" contains "FOLLOW" so we must check UNFOLLOW first,
        # and use word-boundary style matching.
        if re.search(r">\s*unfollow\s*<", page, re.IGNORECASE) and '/follow/remove/' in page:
            return "Yes"

        if re.search(r">\s*follow\s*<", page, re.IGNORECASE) and '/follow/add/' in page:
            return "No"

        """
        // --- FOLLOWING ---
        // UNFOLLOW text + /follow/remove/ ka matlab user already follow kar raha hai
        """
        """
        // --- FALLBACKS ---
        // Agar structure badal jaye lekin text same ho to bhi detection sahi chalegi
        """
        if '/follow/remove/' in page:
            return "Yes"

        if '/follow/add/' in page:
            return "No"

        if re.search(r"\bunfollow\b", page, re.IGNORECASE):
            return "Yes"

        if re.search(r"\bfollow\b", page, re.IGNORECASE):
            return "No"

        return ""

    except Exception:
        return ""

def scrape_recent_post(driver, nickname:str)->dict:
    post_url=f"https://damadam.pk/profile/public/{nickname}"
    try:
        driver.get(post_url)
        try:
            WebDriverWait(driver,5).until(EC.presence_of_element_located((By.CSS_SELECTOR,"article.mbl")))
        except TimeoutException:
            return {'LPOST':'','LDATE-TIME':''}

        recent_post=driver.find_element(By.CSS_SELECTOR,"article.mbl")
        post_data={'LPOST':'','LDATE-TIME':''}

        url_selectors=[
            ("a[href*='/content/']", lambda h: h),
            ("a[href*='/comments/text/']", lambda h: h),
            ("a[href*='/comments/image/']", lambda h: h)
        ]
        for selector, formatter in url_selectors:
            try:
                link=recent_post.find_element(By.CSS_SELECTOR, selector)
                href=link.get_attribute('href')
                if href:
                    formatted=formatter(href)
                    if formatted:
                        post_data['LPOST']=formatted
                        break
            except Exception:
                continue

        time_selectors=["span[itemprop='datePublished']","time[itemprop='datePublished']","span.cxs.cgy","time"]
        for sel in time_selectors:
            try:
                time_elem=recent_post.find_element(By.CSS_SELECTOR, sel)
                if time_elem.text.strip():
                    post_data['LDATE-TIME']=parse_post_timestamp(time_elem.text.strip())
                    break
            except Exception:
                continue
        return post_data
    except Exception:
        return {'LPOST':'','LDATE-TIME':''}

class AdaptiveDelay:
    def __init__(self,mn,mx): self.base_min=mn; self.base_max=mx; self.min_delay=mn; self.max_delay=mx; self.hits=0; self.last=time.time()
    def on_success(self):
        if self.hits: self.hits-=1
        if time.time()-self.last>10:
            self.min_delay=max(self.base_min,self.min_delay*0.95); self.max_delay=max(self.base_max,self.max_delay*0.95); self.last=time.time()
    def on_rate_limit(self):
        self.hits+=1; factor=1+min(0.2*self.hits,1.0)
        self.min_delay=min(3.0,self.min_delay*factor); self.max_delay=min(6.0,self.max_delay*factor)
    def on_batch(self):
        self.min_delay=min(3.0,max(self.base_min,self.min_delay*1.1)); self.max_delay=min(6.0,max(self.base_max,self.max_delay*1.1))
    def sleep(self): time.sleep(random.uniform(self.min_delay,self.max_delay))

adaptive=AdaptiveDelay(MIN_DELAY,MAX_DELAY)

# ==================== BROWSER & LOGIN ====================

def setup_browser():
    try:
        opts=Options(); opts.add_argument("--headless=new"); opts.add_argument("--window-size=1920,1080"); opts.add_argument("--disable-blink-features=AutomationControlled")
        opts.add_experimental_option('excludeSwitches',['enable-automation']); opts.add_experimental_option('useAutomationExtension',False)
        opts.add_argument("--no-sandbox"); opts.add_argument("--disable-dev-shm-usage"); opts.add_argument("--disable-gpu")
        opts.add_argument("--log-level=3")  # Suppress DevTools/Chrome noise
        driver=None
        if CHROMEDRIVER_PATH and os.path.exists(CHROMEDRIVER_PATH):
            service = Service(executable_path=CHROMEDRIVER_PATH)
            driver = webdriver.Chrome(service=service, options=opts)
        else:
            driver = webdriver.Chrome(options=opts)
        driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)
        driver.execute_script("Object.defineProperty(navigator,'webdriver',{get:()=>undefined})")
        return driver
    except Exception as e:
        log_msg(f"Browser error: {e}"); return None

def save_cookies(driver):
    try:
        import pickle
        with open(COOKIE_FILE,'wb') as f: pickle.dump(driver.get_cookies(), f)
    except: pass

def load_cookies(driver):
    try:
        import pickle, os
        if not os.path.exists(COOKIE_FILE): return False
        with open(COOKIE_FILE,'rb') as f: cookies=pickle.load(f)
        for c in cookies:
            try: driver.add_cookie(c)
            except: pass
        return True
    except: return False

def login(driver)->bool:
    try:
        driver.get(HOME_URL); time.sleep(2)
        if load_cookies(driver): driver.refresh(); time.sleep(3); 
        if 'login' not in driver.current_url.lower(): return True
        driver.get(LOGIN_URL); time.sleep(3)
        for label,u,p in [("Account 1",USERNAME,PASSWORD),("Account 2",USERNAME_2,PASSWORD_2)]:
            if not u or not p: continue
            try:
                nick=WebDriverWait(driver,8).until(EC.presence_of_element_located((By.CSS_SELECTOR,"#nick, input[name='nick']")))
                try: pw=driver.find_element(By.CSS_SELECTOR,"#pass, input[name='pass']")
                except: pw=WebDriverWait(driver,8).until(EC.presence_of_element_located((By.CSS_SELECTOR,"input[type='password']")))
                btn=driver.find_element(By.CSS_SELECTOR,"button[type='submit'], form button")
                nick.clear(); nick.send_keys(u); time.sleep(0.5)
                pw.clear(); pw.send_keys(p); time.sleep(0.5)
                btn.click(); time.sleep(4)
                if 'login' not in driver.current_url.lower(): save_cookies(driver); return True
            except: continue
        return False
    except Exception as e:
        log_msg(f"Login error: {e}"); return False

# ==================== GOOGLE SHEETS ====================

def gsheets_client():
    if not GOOGLE_SHEET_URL:
        print("[ERROR] GOOGLE_SHEET_URL is not set."); sys.exit(1)
    scope=["https://www.googleapis.com/auth/spreadsheets","https://www.googleapis.com/auth/drive"]
    try:
        cred_path = _normalize_cred_path(GOOGLE_APPLICATION_CREDENTIALS)
        fallback_path = os.path.join(SCRIPT_DIR, 'credentials.json')
        chosen_path = None
 
        if cred_path and os.path.exists(cred_path):
            chosen_path = cred_path
        elif os.path.exists(fallback_path):
            chosen_path = fallback_path
 
        if chosen_path:
            cred = Credentials.from_service_account_file(chosen_path, scopes=scope)
        else:
            if not GOOGLE_CREDENTIALS_RAW:
                print(
                    "[ERROR] Google credentials not found. "
                    f"Checked GOOGLE_APPLICATION_CREDENTIALS: {cred_path or '(empty)'} and fallback: {fallback_path}. "
                    "Also GOOGLE_CREDENTIALS_JSON is missing."
                )
                sys.exit(1)
            cred = Credentials.from_service_account_info(json.loads(GOOGLE_CREDENTIALS_RAW), scopes=scope)
        return gspread.authorize(cred)
    except Exception as e:
        print(f"[ERROR] Google auth failed: {e}"); sys.exit(1)

class Sheets:
    def __init__(self, client):
        self.client=client; self.ss=client.open_by_url(GOOGLE_SHEET_URL)
        self.tags_mapping={}
        self.ws=self._get_or_create("ProfilesTarget", cols=len(COLUMN_ORDER))
        self.target=self._get_or_create("Target", cols=4)
        self.tags_sheet=self._get_sheet_if_exists("Tags")
        # Ensure headers for ProfilesTarget
        try:
            vals = self.ws.get_all_values()
            if not vals or not vals[0] or all(not c for c in vals[0]):
                log_msg("Initializing ProfilesTarget headers...")
                self.ws.append_row(COLUMN_ORDER)
        except Exception as e:
            log_msg(f"Header init failed: {e}")
        # Ensure headers for Target sheet
        try:
            tvals = self.target.get_all_values()
            if not tvals or not tvals[0] or all(not c for c in tvals[0]):
                log_msg("Initializing Target headers...")
                self.target.append_row(["Nickname","Status","Remarks","Source"])
        except Exception as e:
            log_msg(f"Target header init failed: {e}")
        # Dashboard worksheet
        try:
            self.dashboard = self._get_or_create("Dashboard", cols=11)
            dvals = self.dashboard.get_all_values()
            expected = ["Run#","Timestamp","Profiles","Success","Failed","New","Updated","Unchanged","Trigger","Start","End"]
            if not dvals or dvals[0] != expected:
                self.dashboard.clear(); self.dashboard.append_row(expected)
        except Exception as e:
            log_msg(f"Dashboard setup failed: {e}")
        self._migrate_profiles_target_columns()
        self._load_existing(); self._load_tags_mapping(); self.normalize_target_statuses()

    def apply_quantico_font(self):
        try:
            sheets = self.ss.worksheets()
            reqs = []
            for ws in sheets:
                reqs.append({
                    "repeatCell": {
                        "range": {
                            "sheetId": ws.id,
                            "startRowIndex": 0,
                            "startColumnIndex": 0,
                        },
                        "cell": {
                            "userEnteredFormat": {
                                "textFormat": {
                                    "fontFamily": "Quantico"
                                }
                            }
                        },
                        "fields": "userEnteredFormat.textFormat.fontFamily",
                    }
                })
            if reqs:
                self.ss.batch_update({"requests": reqs})
                log_msg("Applied Quantico font to all sheets")
        except Exception as e:
            log_msg(f"Quantico font apply failed: {e}")

    def _get_or_create(self,name,cols=20,rows=1000):
        try: return self.ss.worksheet(name)
        except WorksheetNotFound:
            return self.ss.add_worksheet(title=name, rows=rows, cols=cols)

    def _get_sheet_if_exists(self,name):
        try:
            return self.ss.worksheet(name)
        except WorksheetNotFound:
            log_msg(f"{name} sheet not found, skipping optional features")
            return None

    def _migrate_profiles_target_columns(self):
        try:
            headers = self.ws.row_values(1)
            if not headers:
                return
            to_remove = {"ID", "FRIEND", "MEHFIL NAME", "MEHFIL DATE"}
            if not any(h in to_remove for h in headers):
                return

            idxs = []
            for name in to_remove:
                try:
                    idxs.append(headers.index(name))
                except ValueError:
                    pass
            if not idxs:
                return

            reqs = []
            for idx in sorted(set(idxs), reverse=True):
                reqs.append(
                    {
                        "deleteDimension": {
                            "range": {
                                "sheetId": self.ws.id,
                                "dimension": "COLUMNS",
                                "startIndex": idx,
                                "endIndex": idx + 1,
                            }
                        }
                    }
                )
            self.ss.batch_update({"requests": reqs})

            end_col_letter = column_letter(len(COLUMN_ORDER)-1)
            self.ws.update(values=[COLUMN_ORDER], range_name=f"A1:{end_col_letter}1")
            time.sleep(SHEET_WRITE_DELAY)
            log_msg("ProfilesTarget columns migrated (removed ID/FRIEND/MEHFIL)")
        except Exception as e:
            log_msg(f"ProfilesTarget migration failed: {e}")

    def _format(self):
        pass  # Formatting disabled as per user request

    def _load_existing(self):
        self.existing={}
        rows=self.ws.get_all_values()[1:]
        nick_idx = COLUMN_TO_INDEX.get("NICK NAME", 0)
        for i,r in enumerate(rows,start=2):
            if len(r) > nick_idx and r[nick_idx].strip():
                self.existing[r[nick_idx].strip().lower()]={'row':i,'data':r}
        log_msg(f"Loaded {len(self.existing)} existing")

    def _load_tags_mapping(self):
        self.tags_mapping={}
        if not self.tags_sheet:
            return
        try:
            all_values=self.tags_sheet.get_all_values()
            if not all_values or len(all_values)<2:
                return
            headers=all_values[0]
            for col_idx, header in enumerate(headers):
                tag_name=clean_data(header)
                if not tag_name:
                    continue
                for row in all_values[1:]:
                    if col_idx < len(row):
                        nickname=row[col_idx].strip()
                        if nickname:
                            key=nickname.lower()
                            if key in self.tags_mapping:
                                if tag_name not in self.tags_mapping[key]:
                                    self.tags_mapping[key]+=f", {tag_name}"
                            else:
                                self.tags_mapping[key]=tag_name
            log_msg(f"Loaded {len(self.tags_mapping)} tags")
        except Exception as e:
            log_msg(f"Tags load failed: {e}")

    def _highlight(self,row_idx,indices):
        return  # Formatting disabled as per user request

    def _add_notes(self,row_idx,indices,before,new_vals):
        if not indices: return
        reqs=[]
        for idx in indices:
            note=f"Before: {before.get(COLUMN_ORDER[idx], '')}\nAfter: {new_vals[idx]}"
            reqs.append({"updateCells":{"range":{"sheetId":self.ws.id,"startRowIndex":row_idx-1,"endRowIndex":row_idx,"startColumnIndex":idx,"endColumnIndex":idx+1},"rows":[{"values":[{"note":note}]}],"fields":"note"}})
        if reqs: self.ss.batch_update({"requests":reqs})

    def update_target_status(self,row,status,remarks):
        lower = (status or "").lower().strip()
        if lower.startswith('pending') or lower == TARGET_STATUS_PENDING.lower():
            status = TARGET_STATUS_PENDING
        elif lower.startswith('done') or lower.startswith('complete') or lower == TARGET_STATUS_DONE.lower():
            status = TARGET_STATUS_DONE
        elif lower.startswith('error') or lower.startswith('unverified') or lower.startswith('suspended') or lower.startswith('banned') or lower == TARGET_STATUS_ERROR.lower():
            status = TARGET_STATUS_ERROR
        # API quota handling
        for attempt in range(3):
            try:
                self.target.update(values=[[status]], range_name=f"B{row}")
                self.target.update(values=[[remarks]], range_name=f"C{row}")
                time.sleep(SHEET_WRITE_DELAY)
                break
            except APIError as e:
                if '429' in str(e):
                    log_msg('[API QUOTA] 429 error: Write quota exceeded, sleeping 60s...')
                    time.sleep(60)
                else:
                    raise

    def update_dashboard(self, metrics:dict):
        try:
            row=[
                metrics.get("Run Number",1),
                metrics.get("Last Run", get_pkt_time().strftime("%d-%b-%y %I:%M %p")),
                metrics.get("Profiles Processed",0),
                metrics.get("Success",0),
                metrics.get("Failed",0),
                metrics.get("New Profiles",0),
                metrics.get("Updated Profiles",0),
                metrics.get("Unchanged Profiles",0),
                metrics.get("Trigger", os.getenv('GITHUB_EVENT_NAME','manual')),
                metrics.get("Start", get_pkt_time().strftime("%d-%b-%y %I:%M %p")),
                metrics.get("End", get_pkt_time().strftime("%d-%b-%y %I:%M %p")),
            ]
            self.dashboard.append_row(row)
        except Exception as e:
            log_msg(f"Dashboard update failed: {e}")

    def normalize_target_statuses(self):
        try:
            vals=self.target.get_all_values()
            if not vals or len(vals)<2: return
            updates=[]
            for idx,row in enumerate(vals[1:],start=2):
                if len(row)<2: continue
                status=row[1].strip()
                lower=status.lower()
                new_status=None
                if ("pending" in lower):
                    if status != TARGET_STATUS_PENDING: new_status = TARGET_STATUS_PENDING
                elif ("done" in lower) or ("complete" in lower):
                    if status != TARGET_STATUS_DONE: new_status = TARGET_STATUS_DONE
                elif ("error" in lower):
                    if status != TARGET_STATUS_ERROR: new_status = TARGET_STATUS_ERROR
                elif status:
                    new_status = TARGET_STATUS_PENDING
                if new_status:
                    updates.append((idx,new_status))
            for row_idx,val in updates:
                self.target.update(values=[[val]], range_name=f"B{row_idx}")
                time.sleep(SHEET_WRITE_DELAY)
        except Exception as e:
            log_msg(f"Normalize statuses failed: {e}")

    def write_profile(self, profile:dict, old_row:int|None=None):
        nickname=(profile.get("NICK NAME") or "").strip()
        if not nickname: return {"status":"error","error":"Missing nickname","changed_fields":[]}
        if profile.get("LAST POST TIME"): profile["LAST POST TIME"]=convert_relative_date_to_absolute(profile["LAST POST TIME"])
        profile["DATETIME SCRAP"]=get_pkt_time().strftime("%d-%b-%y %I:%M %p")
        tags_val=self.tags_mapping.get(nickname.lower())
        if tags_val:
            profile["TAGS"]=tags_val
        vals=[]
        for c in COLUMN_ORDER:
            v=clean_data(profile.get(c,""))
            vals.append(v)
        key=nickname.lower(); ex=self.existing.get(key)
        if ex:
            before={COLUMN_ORDER[i]:(ex['data'][i] if i<len(ex['data']) else "") for i in range(len(COLUMN_ORDER))}
            changed=[i for i,col in enumerate(COLUMN_ORDER) if col not in HIGHLIGHT_EXCLUDE_COLUMNS and (before.get(col,"" ) or "") != (vals[i] or "")]
            # Update in place (overwrite row)
            rownum=ex['row']
            end_col_letter = column_letter(len(COLUMN_ORDER)-1)
            self.ws.update(values=[vals], range_name=f"A{rownum}:{end_col_letter}{rownum}")
            if changed:
                self._add_notes(rownum,changed,before,vals)
            self.existing[key]={'row':rownum,'data':vals}
            status="updated" if changed else "unchanged"
            result={"status":status,"changed_fields":[COLUMN_ORDER[i] for i in changed]}
        else:
            self.ws.append_row(vals)
            last_row=len(self.ws.get_all_values())
            self.existing[key]={'row':last_row,'data':vals}
            result={"status":"new","changed_fields":list(COLUMN_ORDER)}
        time.sleep(SHEET_WRITE_DELAY)
        return result

# ==================== TARGET PROCESSING ====================

def get_pending_targets(sheets:Sheets):
    rows=sheets.target.get_all_values()[1:]
    out=[]
    for idx,row in enumerate(rows,start=2):
        nick=(row[0] if len(row)>0 else '').strip()
        status=(row[1] if len(row)>1 else '').strip()
        source=(row[3] if len(row)>3 else 'Target').strip() or 'Target'
        norm=status.lower()
        is_pending=(not status) or (status == TARGET_STATUS_PENDING) or ("pending" in norm)
        if nick and is_pending:
            out.append({'nickname':nick,'row':idx,'source':source})
    return out

# ==================== PROFILE SCRAPING ====================

def scrape_profile(driver, nickname:str)->dict|None:
    url=f"https://damadam.pk/users/{nickname}/"
    try:
        log_msg(f"[SCRAPING] {nickname}")
        driver.get(url)
        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR,"h1.cxl.clb.lsp")))

        page_source=driver.page_source
        now=get_pkt_time()
        suspend_reason=detect_suspension_reason(page_source)
        data={
            "NICK NAME":nickname,
            "TAGS":"",
            "CITY":"",
            "GENDER":"",
            "MARRIED":"",
            "AGE":"",
            "JOINED":"",
            "FOLLOWERS":"",
            "STATUS":"Normal",
            "POSTS":"",
            "INTRO":"",
            "SOURCE":"Target",
            "DATETIME SCRAP":now.strftime("%d-%b-%y %I:%M %p"),
            "LAST POST":"",
            "LAST POST TIME":"",
            "IMAGE":"",
            "PROFILE LINK":url.rstrip('/'),
            "POST URL":f"https://damadam.pk/profile/public/{nickname}",
        }

        if suspend_reason:
            data['STATUS'] = 'Banned'
            data['INTRO'] = "Account Suspended"[:250]
            data['__skip_reason'] = 'Account Suspended'
            return data

        if 'account suspended' in page_source.lower():
            data['STATUS'] = 'Banned'
            data['__skip_reason'] = 'Account Suspended'
            return data
        elif (
            re.search(r">\s*unverified\s*user\s*<", page_source, re.IGNORECASE)
            or 'background:tomato' in page_source
            or 'style="background:tomato"' in page_source.lower()
        ):
            data['STATUS'] = 'Unverified'
            data['__skip_reason'] = 'skipped coz of unverified user'
            return data
        else:
            try:
                driver.find_element(By.CSS_SELECTOR, "div[style*='tomato']")
                data['STATUS'] = 'Unverified'
                data['__skip_reason'] = 'skipped coz of unverified user'
                return data
            except Exception:
                data['STATUS'] = 'Normal'

        for sel in ["span.cl.sp.lsp.nos","span.cl",".ow span.nos"]:
            try:
                intro=driver.find_element(By.CSS_SELECTOR, sel)
                if intro.text.strip():
                    data['INTRO']=clean_text(intro.text)
                    break
            except Exception:
                pass

        fields={'City:':'CITY','Gender:':'GENDER','Married:':'MARRIED','Age:':'AGE','Joined:':'JOINED'}
        for label,key in fields.items():
            try:
                elem=driver.find_element(By.XPATH,f"//b[contains(text(), '{label}')]/following-sibling::span[1]")
                value=elem.text.strip()
                if not value: continue
                if key=='JOINED':
                    data[key]=convert_relative_date_to_absolute(value)
                elif key=='GENDER':
                    low=value.lower()
                    if 'female' in low:
                        data[key] = 'Female'
                    elif 'male' in low:
                        data[key] = 'Male'
                    else:
                        data[key] = ''
                elif key=='MARRIED':
                    low=value.lower()
                    if low in {'yes','married'}:
                        data[key] = 'Yes'
                    elif low in {'no','single','unmarried'}:
                        data[key] = 'No'
                    else:
                        data[key] = ''
                else:
                    data[key]=clean_data(value)
            except Exception:
                continue

        for sel in ["span.cl.sp.clb",".cl.sp.clb"]:
            try:
                followers=driver.find_element(By.CSS_SELECTOR, sel)
                match=re.search(r'(\d+)', followers.text)
                if match:
                    data['FOLLOWERS']=match.group(1)
                    break
            except Exception:
                pass

        for sel in ["a[href*='/profile/public/'] button div:first-child","a[href*='/profile/public/'] button div"]:
            try:
                posts=driver.find_element(By.CSS_SELECTOR, sel)
                match=re.search(r'(\d+)', posts.text)
                if match:
                    data['POSTS']=match.group(1)
                    break
            except Exception:
                pass

        for sel in ["img[src*='avatar-imgs']","img[src*='avatar']","div[style*='whitesmoke'] img[src*='cloudfront.net']"]:
            try:
                img=driver.find_element(By.CSS_SELECTOR, sel)
                src=img.get_attribute('src')
                if src and ('avatar' in src or 'cloudfront.net' in src):
                    data['IMAGE']=src.replace('/thumbnail/','/')
                    break
            except Exception:
                pass

        if data.get('POSTS') and data['POSTS']!='0':
            time.sleep(1)
            post_data=scrape_recent_post(driver, nickname)
            data['LAST POST']=clean_data(post_data.get('LPOST',''))
            data['LAST POST TIME']=post_data.get('LDATE-TIME','')

        log_msg(f"[OK] Extracted: {data['GENDER']}, {data['CITY']}, Posts: {data['POSTS']}, Friend: {data.get('FRIEND','')}")

        return data
    except TimeoutException:
        log_msg(f"[TIMEOUT] Timeout while scraping {nickname}")
        return None
    except WebDriverException:
        log_msg(f"[BROWSER_ERROR] Browser issue while scraping {nickname}")
        return None
    except Exception as e:
        log_msg(f"[ERROR] Error scraping {nickname}: {str(e)[:60]}")
        return None

# ==================== MAIN ENTRY ====================

def main():
    parser = argparse.ArgumentParser(add_help=True)
    parser.add_argument("--batch-size", type=int, default=None)
    parser.add_argument("--max-profiles", type=int, default=None, help="Max profiles to scrape (0 = all)")
    parser.add_argument("--profiles-to-scrape", dest="max_profiles", type=int, default=None, help="Alias for --max-profiles (0 = all)")
    parser.add_argument("--apply-font", action="store_true", help="Apply Quantico font to all Google Sheets")
    parser.add_argument("--apply-font-only", action="store_true", help="Apply Quantico font to all Google Sheets and exit")
    parser.add_argument("--no-apply-font", action="store_true", help="Do not apply Quantico font")
    args = parser.parse_args()

    is_interactive = sys.stdin.isatty() and not os.getenv('GITHUB_ACTIONS')

    if args.apply_font_only:
        if args.batch_size is None:
            args.batch_size = BATCH_SIZE
        if args.max_profiles is None:
            args.max_profiles = 0

    if args.batch_size is None:
        if is_interactive:
            raw = input(f"Batch Size (default {BATCH_SIZE}): ").strip()
            args.batch_size = int(raw) if raw else BATCH_SIZE
        else:
            args.batch_size = BATCH_SIZE

    if args.max_profiles is None:
        if is_interactive:
            raw = input("Profiles to scrape (0=All, default 0): ").strip()
            args.max_profiles = int(raw) if raw else 0
        else:
            args.max_profiles = MAX_PROFILES_PER_RUN

    os.environ['BATCH_SIZE'] = str(args.batch_size)
    os.environ['MAX_PROFILES_PER_RUN'] = str(args.max_profiles)

    header = Table.grid(padding=(0, 2))
    header.add_column(justify="left")
    header.add_row("DamaDam Target Bot", "v3.2.1")
    header.add_row("Batch Size", str(args.batch_size))
    header.add_row("Profiles", "All" if args.max_profiles == 0 else str(args.max_profiles))
    console.print(Panel(header, title="Run Config", border_style="magenta"))
    print("\n"+"="*70)
    print("  [TARGET] DamaDam Target Bot v3.2.1 (Single File)")
    print("="*70)
    if not USERNAME or not PASSWORD: print("[ERROR] Missing DAMADAM_USERNAME / DAMADAM_PASSWORD"); sys.exit(1)
    log_msg("Connecting to Google Sheets...")
    if IS_CI:
        client = gsheets_client(); sheets = Sheets(client)
    else:
        with Status("ðŸ”Œ Connecting to Google Sheets...", console=console, spinner="dots"):
            client = gsheets_client(); sheets = Sheets(client)

    apply_font = (not args.no_apply_font) and (args.apply_font_only or args.apply_font or APPLY_FONT_FORMATTING or True)
    if apply_font:
        if IS_CI:
            sheets.apply_quantico_font()
        else:
            with Status("ðŸ”¤ Applying Quantico font...", console=console, spinner="dots"):
                sheets.apply_quantico_font()

    if args.apply_font_only:
        log_msg("Font formatting complete (apply-font-only). Exiting.")
        return

    log_msg("Setting up browser...")
    if IS_CI:
        driver = setup_browser()
    else:
        with Status("ðŸŒ Launching Chrome...", console=console, spinner="dots"):
            driver = setup_browser()
    if not driver: print("[ERROR] Browser setup failed"); sys.exit(1)
    try:
        log_msg("Logging in...")
        if IS_CI:
            ok = login(driver)
        else:
            with Status("ðŸ” Logging in...", console=console, spinner="dots"):
                ok = login(driver)
        if not ok: print("[ERROR] Login failed"); driver.quit(); sys.exit(1)

        log_msg("Fetching pending targets...")
        if IS_CI:
            targets = get_pending_targets(sheets)
        else:
            with Status("ðŸ“¥ Reading Target sheet...", console=console, spinner="dots"):
                targets = get_pending_targets(sheets)
        if not targets: log_msg("No pending targets."); return
        # Enforce max profiles strictly
        to_process = targets[:args.max_profiles] if args.max_profiles > 0 else targets
        success=failed=suspended_count=0
        run_stats={"new":0,"updated":0,"unchanged":0}
        start_time=time.time(); run_started=get_pkt_time()
        trigger_type="Scheduled" if os.getenv('GITHUB_EVENT_NAME','').lower()=='schedule' else "Manual"
        current_target=None
        log_msg(f"Starting scrape of {len(to_process)} profiles...")
        processed_count = 0
        try:
            with Progress(
                SpinnerColumn(style="cyan"),
                TextColumn("{task.description}"),
                BarColumn(bar_width=30),
                TextColumn("{task.completed}/{task.total}"),
                TimeElapsedColumn(),
                TimeRemainingColumn(),
                console=console,
                transient=False,
            ) as progress:
                task_id = progress.add_task("Scraping profiles", total=len(to_process))
                while processed_count < len(to_process):
                    t = to_process[processed_count]
                    current_target = t
                    nick = t['nickname']; row = t['row']; source = t.get('source','Target') or 'Target'
                    eta = calculate_eta(processed_count, len(to_process), start_time)
                    progress.update(task_id, description=f"[{eta}] {nick}")
                    try:
                        prof = scrape_profile(driver, nick)
                        if not prof:
                            raise RuntimeError("Profile scrape failed")
                        prof['SOURCE'] = source

                        skip_reason = prof.get('__skip_reason')
                        if skip_reason:
                            sheets.write_profile(prof, old_row=row)
                            sheets.update_target_status(row, "Error", f"{skip_reason} @ {get_pkt_time().strftime('%I:%M %p')}")
                            failed += 1
                        else:
                            result = sheets.write_profile(prof, old_row=row)
                            status = result.get("status","error") if result else "error"
                            if status in {"new","updated","unchanged"}:
                                success += 1
                                run_stats[status] += 1
                                sheets.update_target_status(row, "Done", f"{status} @ {get_pkt_time().strftime('%I:%M %p')}")
                            else:
                                raise RuntimeError(result.get("error","Write failed") if result else "Write failed")
                    except Exception as e:
                        sheets.update_target_status(row, "Pending", f"Retry needed: {e}")
                        failed += 1
                    current_target = None
                    processed_count += 1
                    progress.advance(task_id)
                    if args.batch_size > 0 and processed_count % args.batch_size == 0 and processed_count < len(to_process):
                        adaptive.on_batch(); time.sleep(3)
                    adaptive.sleep()
        except KeyboardInterrupt:
            print("\n" + "-"*70)
            log_msg("Run interrupted by user")
            if current_target:
                sheets.update_target_status(current_target['row'], "Pending", f"Interrupted @ {get_pkt_time().strftime('%I:%M %p')}")
        except Exception as fatal:
            print("\n" + "-"*70)
            log_msg(f"Fatal error: {fatal}")
            if current_target:
                sheets.update_target_status(current_target['row'], "Pending", f"Run error: {fatal}")
            return
        print("-"*70)
        log_msg(f"[COMPLETE] Run completed: {success} success, {failed} failed, {suspended_count} suspended")
        sheets.update_dashboard({
            "Run Number":1,
            "Last Run": get_pkt_time().strftime("%d-%b-%y %I:%M %p"),
            "Profiles Processed": len(targets),
            "Success": success,
            "Failed": failed,
            "New Profiles": run_stats.get('new',0),
            "Updated Profiles": run_stats.get('updated',0),
            "Unchanged Profiles": run_stats.get('unchanged',0),
            "Trigger": trigger_type,
            "Start": run_started.strftime("%d-%b-%y %I:%M %p"),
            "End": get_pkt_time().strftime("%d-%b-%y %I:%M %p"),
        })
        print("="*70)
    finally:
        try: driver.quit()
        except: pass

if __name__=='__main__':
    main()




















