# ğŸ¤– DamaDam Scraper â€” v2.100.0.18

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://python.org)
[![Phase 1](https://img.shields.io/badge/Phase%201-Complete-brightgreen.svg)]()
[![Target Mode](https://github.com/net2t/DD-CMS-Final/actions/workflows/scrape-target.yml/badge.svg)](https://github.com/net2t/DD-CMS-Final/actions/workflows/scrape-target.yml)
[![Online Mode](https://github.com/net2t/DD-CMS-Final/actions/workflows/scrape-online.yml/badge.svg)](https://github.com/net2t/DD-CMS-Final/actions/workflows/scrape-online.yml)

Scrapes user profiles from **DamaDam.pk** and saves everything to Google Sheets.
Runs automatically in the cloud via GitHub Actions â€” no server, no PC needed.

---

## How It Works

```
You add usernames to the RunList sheet
         â†“
GitHub Actions wakes up on schedule
         â†“
Opens Chrome (invisibly, in the cloud)
         â†“
Logs into DamaDam with your account
         â†“
Visits each profile, reads the data
         â†“
Writes everything to your Google Sheet
         â†“
Done â€” just check the sheet!
```

**Two modes:**
- **Target Mode** â€” You give a list of usernames, it scrapes them
- **Online Mode** â€” Checks who is online right now and scrapes those profiles (runs every 15 min automatically)

---

## Project Structure

```
DD-CMS-Final/
â”‚
â”œâ”€â”€ main.py                        â† Run this to start everything
â”œâ”€â”€ requirements.txt               â† Python packages
â”œâ”€â”€ .env.sample                    â† Copy this to .env and fill in your details
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config_common.py           â† All main settings (timeouts, columns, delays)
â”‚   â”œâ”€â”€ config_manager.py          â† Loads the right config per mode
â”‚   â”œâ”€â”€ config_online.py           â† Online mode overrides
â”‚   â”œâ”€â”€ config_target.py           â† Target mode overrides
â”‚   â”œâ”€â”€ config_test.py             â† Test mode settings
â”‚   â”œâ”€â”€ config_mehfil.py           â† Mehfil phase settings (future)
â”‚   â”œâ”€â”€ config_posts.py            â† Posts phase settings (future)
â”‚   â””â”€â”€ selectors.py               â† CSS/XPath selectors for the website
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ browser_manager.py         â† Starts and controls Chrome
â”‚   â”œâ”€â”€ browser_context.py         â† Safe browser lifecycle wrapper
â”‚   â”œâ”€â”€ login_manager.py           â† Login with cookie + fallback system
â”‚   â””â”€â”€ run_context.py             â† Shared state for one run
â”‚
â”œâ”€â”€ phases/
â”‚   â”œâ”€â”€ phase_profile.py           â† Phase 1 router (online/target/test)
â”‚   â”œâ”€â”€ phase_posts.py             â† Phase 2 stub (future)
â”‚   â”œâ”€â”€ phase_mehfil.py            â† Phase 3 stub (future)
â”‚   â””â”€â”€ profile/
â”‚       â”œâ”€â”€ target_mode.py         â† Core scraper + target list runner
â”‚       â””â”€â”€ online_mode.py         â† Online users fetcher
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ sheets_manager.py          â† All Google Sheets operations
â”‚   â”œâ”€â”€ sheets_batch_writer.py     â† Batch writer (faster writes)
â”‚   â”œâ”€â”€ ui.py                      â† Terminal display and logging
â”‚   â”œâ”€â”€ url_builder.py             â† Builds profile URLs
â”‚   â”œâ”€â”€ dynamic_dashboard.py       â† Dashboard update logic
â”‚   â””â”€â”€ metrics.py                 â† Run performance tracking
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ scrape-online.yml          â† Runs every 15 minutes (auto)
â”‚   â””â”€â”€ scrape-target.yml          â† Manual trigger only
â”‚
â”œâ”€â”€ Dashboard/
â”‚   â”œâ”€â”€ index.html                 â† Web dashboard UI
â”‚   â”œâ”€â”€ styles.css
â”‚   â””â”€â”€ app.js
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ guides/                    â† Setup and troubleshooting guides
    â””â”€â”€ meta/
        â””â”€â”€ CHANGELOG.md           â† Version history
```

---

## Quick Start (Local)

### Requirements
- Python 3.9+
- Google Chrome
- Google account (for Sheets API)
- DamaDam.pk account (2 accounts recommended)

### Step 1 â€” Clone and install

```bash
git clone https://github.com/net2t/DD-CMS-Final.git
cd DD-CMS-Final
pip install -r requirements.txt
```

### Step 2 â€” Create your `.env` file

```bash
copy .env.sample .env      # Windows
cp .env.sample .env        # Mac/Linux
```

Open `.env` and fill in your credentials (see Configuration section below).

### Step 3 â€” Set up Google Sheets

1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create a project â†’ Enable **Google Sheets API** + **Google Drive API**
3. Create a **Service Account** â†’ Download `credentials.json`
4. Place `credentials.json` in the project folder
5. Create a Google Sheet and **share it** with the service account email
6. Your sheet needs these tabs (auto-created if missing): `Profiles`, `RunList`, `OnlineLog`, `Dashboard`

### Step 4 â€” Test

```bash
python main.py test --max-profiles 3
```

---

## Configuration

Your `.env` file:

```env
# DamaDam accounts
DAMADAM_USERNAME=your_username
DAMADAM_PASSWORD=your_password
DAMADAM_USERNAME_2=backup_username     # Recommended
DAMADAM_PASSWORD_2=backup_password

# Google Sheet (full URL from browser)
GOOGLE_SHEET_URL=https://docs.google.com/spreadsheets/d/YOUR_ID/edit

# Leave these as-is
GOOGLE_APPLICATION_CREDENTIALS=credentials.json
PAGE_LOAD_TIMEOUT=20
SHEET_WRITE_DELAY=1.0

# Speed setting: false = fast (recommended), true = fetches more last post data
LAST_POST_FETCH_PUBLIC_PAGE=false
```

| Setting | Default | Description |
|---|---|---|
| `MAX_PROFILES_PER_RUN` | `0` | Max profiles per run (0 = unlimited) |
| `BATCH_SIZE` | `20` | Profiles per batch before pausing |
| `MIN_DELAY` / `MAX_DELAY` | `0.3` / `0.5` | Wait between profiles (seconds) |
| `PAGE_LOAD_TIMEOUT` | `20` | Give up loading after this many seconds |
| `SHEET_WRITE_DELAY` | `1.0` | Wait between sheet writes (avoids 429 errors) |
| `LAST_POST_FETCH_PUBLIC_PAGE` | `false` | Also check public page for last post data |

---

## Running the Scraper

```bash
# Scrape your RunList
python main.py target

# Limit to 50 profiles
python main.py target --max-profiles 50

# Scrape currently online users
python main.py online --max-profiles 30

# Quick test (3 profiles)
python main.py test
```

**RunList sheet format:**

| A: NICKNAME | B: STATUS | C: REMARKS | D: SKIP |
|---|---|---|---|
| user123 | âš¡ Pending | | |
| user456 | Done ğŸ’€ | Updated: 20-Feb-26 | |
| skipme | | | YES |

---

## GitHub Actions (Auto Mode)

### Setup â€” Add these 6 Secrets

Go to your repo â†’ **Settings** â†’ **Secrets and variables** â†’ **Actions** â†’ **New repository secret**

| Secret Name | Value |
|---|---|
| `DAMADAM_USERNAME` | Your DamaDam username |
| `DAMADAM_PASSWORD` | Your DamaDam password |
| `DAMADAM_USERNAME_2` | Backup username |
| `DAMADAM_PASSWORD_2` | Backup password |
| `GOOGLE_SHEET_URL` | Full URL of your Google Sheet |
| `GOOGLE_CREDENTIALS_JSON` | Paste entire content of `credentials.json` |

### Schedules

| Workflow | When | What |
|---|---|---|
| Online Mode | Every 15 minutes (auto) | Scrapes currently online users |
| Target Mode | Manual only | Processes your RunList |

### Manual Trigger

Repo â†’ **Actions** â†’ **Target Mode** â†’ **Run workflow** â†’ Set max profiles â†’ **Run workflow**

---

## Phase System

| Phase | Status | What It Does |
|---|---|---|
| Phase 1 â€” Profiles | âœ… Complete | Scrapes full profile data (23 fields) |
| Phase 2 â€” Posts | ğŸ”œ Planned | Scrapes posts from eligible profiles |
| Phase 3 â€” Mehfils | ğŸ”œ Planned | Scrapes group data |
| Phase 4 â€” Comments | ğŸ”œ Future | Scrapes comments |

**Phase 2 Eligibility:** A profile is marked `Ready` if it has fewer than 100 posts and status is Active.

---

## Data Collected Per Profile

ID, Nickname, Tags, City, Gender, Married, Age, Joined, Followers, Status, Posts, Run Mode, Scrape DateTime, Last Post URL, Last Post Time, Profile Image, Profile Link, Public Posts URL, Rank Star, Mehfil Names, Mehfil Links, Mehfil Dates, Phase 2 Eligibility

---

## Troubleshooting

**Login keeps failing**
```bash
del damadam_cookies.pkl
python main.py test --max-profiles 1
```

**Google Sheets 429 error (rate limit)**
Set `SHEET_WRITE_DELAY=2.0` in your `.env`

**No pending targets found in Target mode**
Check `RunList` sheet â€” column B must say exactly `âš¡ Pending`

**GitHub Actions fails immediately**
Make sure all 6 secrets are added. `GOOGLE_CREDENTIALS_JSON` must include the full JSON starting with `{`

---

## Security

Never commit these files â€” they contain your passwords:
- `.env`
- `credentials.json`
- `damadam_cookies.pkl`

These are already in `.gitignore`. For GitHub Actions, always use Secrets.

---

## Author

**Nadeem** Â· [net2outlawzz@gmail.com](mailto:net2outlawzz@gmail.com) Â· [@net2nadeem](https://instagram.com/net2nadeem)
AI Assistant: Claude (Anthropic)
